{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è 1. Environment Setup\n",
        "\n",
        "This first cell installs all the necessary libraries for fine-tuning a large language model using modern, memory-efficient techniques.\n",
        "\n",
        "- **`accelerate`**: A Hugging Face library that simplifies running PyTorch training on any kind of distributed setup.\n",
        "- **`bitsandbytes`**: A key library that enables model quantization, allowing us to load large models like Phi-2 in 8-bit precision, which drastically reduces GPU memory usage.\n",
        "- **`trl`**: The Transformer Reinforcement Learning library, which provides useful tools and trainers for fine-tuning.\n",
        "- **`peft`**: The Parameter-Efficient Fine-Tuning library. This is crucial for using techniques like LoRA (Low-Rank Adaptation), which we will use here.\n",
        "- **`transformers` & `datasets`**: The core Hugging Face libraries for models, tokenizers, and data handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q accelerate -U\n",
        "!pip install -q bitsandbytes -U\n",
        "!pip install -q trl -U\n",
        "!pip install -q peft -U\n",
        "!pip install -q transformers -U\n",
        "!pip install -q datasets -U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• 2. Data Loading and Preprocessing\n",
        "\n",
        "We start by loading a dataset of Amazon product details using `pandas`. The initial data needs to be reshaped for our specific task.\n",
        "\n",
        "1.  **Load Data**: We load the CSV, keeping only the `category`, `about_product`, and `product_name` columns.\n",
        "2.  **Clean Category**: The `category` column contains a breadcrumb trail (e.g., 'Electronics|Computers|Laptops'). We clean this to keep only the most specific, final category.\n",
        "3.  **Restructure for Tasks**: We want to train the model on two related tasks: generating a *product name* and generating a *product description*. We split the original DataFrame into two, rename the columns to a common name (`text`), and add a `task_type` column to each to identify the task. \n",
        "4.  **Combine**: We then concatenate the two DataFrames back into one, creating a unified dataset where each row is an example of a specific task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "df = pd.read_csv('https://github.com/laxmimerit/All-CSV-ML-Data-Files-Download/raw/master/amazon_product_details.csv', usecols=['category', 'about_product', 'product_name'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['category'] = df['category'].apply(lambda x: x.split('|')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "products = df[['category', 'product_name']]\n",
        "description = df[['category', 'about_product']]\n",
        "\n",
        "products = products.rename(columns={'product_name': 'text'})\n",
        "description = description.rename(columns={'about_product': 'text'})\n",
        "\n",
        "products['task_type'] = 'Product Name'\n",
        "description['task_type'] = 'Product Description'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["df = pd.concat([products, description], ignore_index=True)"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìñ 3. Creating the Hugging Face Dataset\n",
        "\n",
        "With our data preprocessed in pandas, we convert it into a Hugging Face `Dataset` object. This format is optimized for use with the `transformers` library. We then shuffle the dataset for randomness and split it into a training set (90%) and a test set (10%) to evaluate our model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.shuffle(seed=0)\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["dataset"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["dataset['test'][2]"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "A quick EDA step is performed to understand the length of our text fields. We estimate the number of tokens for both product names and descriptions and plot histograms. This helps us confirm that product names are very short, while descriptions have a wider range of lengths. This information is useful for selecting a `max_length` for tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.5 times of total words\n",
        "products['text_tokens'] = products['text'].apply(lambda x: len(x.split())*1.5)\n",
        "description['text_tokens'] = description['text'].apply(lambda x: len(x.split())*1.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["products['text_tokens'].hist()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["description['text_tokens'].hist()"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù 5. Creating an Instruction-Based Prompt Template\n",
        "\n",
        "To effectively fine-tune the model, we need to structure our data into a clear prompt format. The `formatting_func` creates an instruction-based prompt for each example. It tells the model exactly what to do (e.g., \"generate a 'Product Description'\"), provides the necessary input (the `Category`), and then provides the expected output. This process, known as instruction fine-tuning, is a highly effective way to teach a model a new task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"\"\"\n",
        "            Given the product category, you need to generate a '{example['task_type']}'.\n",
        "            ### Category: {example['category']}\\n ### {example['task_type']}: {example['text']}\n",
        "\n",
        "            \"\"\"\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(formatting_func(dataset['train'][0]))"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ 6. Loading the Quantized Model and Tokenizer\n",
        "\n",
        "Here, we load the pre-trained `microsoft/phi-2` model. The key parameter is `load_in_8bit=True`, which uses the `bitsandbytes` library to quantize the model's weights to 8-bit integers. This significantly reduces the GPU memory required to load the model.\n",
        "\n",
        "We also load the tokenizer. For causal (decoder-only) models like Phi-2, it's important to set `padding_side='left'`. This ensures that padding tokens are added to the left of the sequence, which prevents the model from getting confused during generation. We also set the pad token to be the same as the end-of-sequence (EOS) token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "base_model_id = \"microsoft/phi-2\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, trust_remote_code=True,\n",
        "                                             torch_dtype=torch.float16, load_in_8bit=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side='left',\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        "    use_fast=False\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úçÔ∏è 7. Tokenizing the Dataset for Causal LM Fine-Tuning\n",
        "\n",
        "We define a function to tokenize our formatted prompts. For fine-tuning a Causal Language Model, the goal is to predict the next token in a sequence. Therefore, the `labels` (what the model tries to predict) are simply a copy of the `input_ids`. The model reads the sequence token by token and learns to predict the subsequent token at each step.\n",
        "\n",
        "This tokenization function is then applied to the entire dataset using `.map()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "max_length = 400\n",
        "\n",
        "def tokenize(prompt):\n",
        "  result = tokenizer(\n",
        "      formatting_func(prompt),\n",
        "      truncation = True,\n",
        "      max_length=max_length,\n",
        "      padding = \"max_length\"\n",
        "  )\n",
        "\n",
        "  result['labels'] = result['input_ids'].copy()\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print(tokenize(dataset['train'][0]))"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["dataset = dataset.map(tokenize)"]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üßê 8. Baseline Inference (Before Fine-Tuning)\n",
        "\n",
        "Before we start training, we test the base model's ability to perform our task. We provide it with a prompt asking for a 'Product Description' for the 'BatteryChargers' category and see what it generates. This provides a crucial baseline, allowing us to clearly see the improvements after fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Given the product category, you need to generate a 'Product Description'.\n",
        "### Category: BatteryChargers\n",
        "### Product Description:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize -> generate -> decode\n",
        "\n",
        "model_input = tokenizer(\n",
        "      eval_prompt,\n",
        "      truncation = True,\n",
        "      max_length=max_length,\n",
        "      padding = \"max_length\",\n",
        "      return_tensors='pt'\n",
        "  ).to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  output = model.generate(**model_input, max_new_tokens=256,\n",
        "                                           repetition_penalty=1.15)\n",
        "  result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è 9. QLoRA Configuration\n",
        "\n",
        "This is the core of our memory-efficient fine-tuning strategy. Instead of training the entire model, we use QLoRA (Quantized Low-Rank Adaptation).\n",
        "\n",
        "1.  **`LoraConfig`**: We define a LoRA configuration using the `peft` library. We specify which parts of the model we want to adapt (the `target_modules`) and set hyperparameters for the small, trainable adapter matrices (`r`, `lora_alpha`).\n",
        "2.  **`get_peft_model`**: This function takes our quantized base model and the LoRA config, and injects the small, trainable adapter layers into the specified modules.\n",
        "3.  **Parameter Check**: The `print_trainable_parameters` function shows the magic of LoRA. We can see that we have frozen the vast majority of the model's 2.8 billion parameters and are only training a tiny fraction (~1%). This is what makes fine-tuning large models feasible on a single GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "\n",
        "target_modules = [\"Wqkv\", \"fc1\", \"fc2\"]\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules = target_modules,\n",
        "    bias = \"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["print_trainable_parameters(model)"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator(gradient_accumulation_steps=1)\n",
        "\n",
        "model = accelerator.prepare_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚ñ∂Ô∏è 10. Training the Model\n",
        "\n",
        "We are now ready to train. We set up the `Trainer` with our LoRA-adapted model and `TrainingArguments`.\n",
        "\n",
        "- **Optimizer**: We use `paged_adamw_8bit`, a special optimizer from `bitsandbytes` that is designed to work efficiently with quantized models.\n",
        "- **Data Collator**: We use `DataCollatorForLanguageModeling` with `mlm=False` (Masked Language Modeling is set to false), which is the standard for Causal Language Model fine-tuning.\n",
        "\n",
        "Finally, `trainer.train()` starts the fine-tuning process, where only the small LoRA adapter weights are updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trainer, Training Arguments, DataCollator\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datetime import datetime\n",
        "\n",
        "project = \"phi2-finetune\"\n",
        "run_name = 'train-dir'\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "args=TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "    )\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args = args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test'],\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ 11. Inference After Fine-Tuning\n",
        "\n",
        "To test our fine-tuned model, we first need to merge the trained adapter weights with the original base model weights.\n",
        "\n",
        "1.  **Load Base Model**: We load the original 8-bit quantized `microsoft/phi-2` model again.\n",
        "2.  **Load PEFT Model**: We use `PeftModel.from_pretrained` to load our saved LoRA adapter from the final checkpoint (`checkpoint-500`) and apply it to the base model.\n",
        "3.  **Generate Text**: We use the exact same prompt as our baseline test. By comparing the new output with the baseline, we can clearly see that the model has learned to follow the instructions and generate a relevant product description, demonstrating the success of our fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    trust_remote_code=True,\n",
        "    load_in_8bit=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        "    trust_remote_code=True,\n",
        "    use_fast=False\n",
        ")\n",
        "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, '/content/train-dir/checkpoint-500')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_prompt = \"\"\"\n",
        "Given the product category, you need to generate a 'Product Description'.\n",
        "### Category: BatteryChargers\n",
        "### Product Description:\n",
        "\"\"\"\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "  output = ft_model.generate(**model_input, max_new_tokens=256,\n",
        "                                           repetition_penalty=1.15)\n",
        "  result = eval_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ 12. Saving the Adapter\n",
        "\n",
        "Finally, we zip the contents of our final checkpoint directory. This creates a single, portable `phi2_qlora_adapter.zip` file containing our trained LoRA adapter, which can be easily shared and loaded later for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!zip -r phi2_qlora_adapter.zip /content/train-dir/checkpoint-500\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 2
}
